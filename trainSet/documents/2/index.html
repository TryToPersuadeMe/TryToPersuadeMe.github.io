<!DOCTYPE html><html lang="ru">  <head>    <meta charset="UTF-8" />    <meta name="viewport" content="width=device-width, initial-scale=1.0" />    <meta name="format-detection" content="telephone=no" />    <meta name="keywords" content="ключевые слова" />    <meta name="description" content="крутой сайт" />    <link href="./resources/favicon.ico" rel="shortcut icon" type="image/x-icon" />    <link rel="stylesheet" href="./css/main.css" />    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>    <title>Train Set - Главная страница</title>  </head>  <body>    <div class="wrapper" id="startPage">      <div class="container responsiveHeader">  <a href="/" class="header__logo">    <picture><source srcset="./resources/images/logotype.svg" type="image/webp"><img src="./resources/images/logotype.svg" alt="логотип Train Set" /></picture>  </a>  <div class="burgerIcon"><span></span></div></div><header class="header">  <div class="container header__container">    <a href="/" class="header__logo">      <picture><source srcset="./resources/images/logotype.svg" type="image/webp"><img src="./resources/images/logotype.svg" alt="логотип Train Set" /></picture>    </a>    <nav class="header__navigation">      <a href="#" class="link">Academy</a>      <a href="#" class="link link_animatedBG">Projeсts</a>      <a href="#" class="link">Contact</a>    </nav>    <div class="header__authorization">      <a href="https://trytopersuademe.github.io/trainSet/authorization/" class="link link_border">Sign in</a>      <a href="#" class="link link_border">Get Started</a>    </div>  </div></header>      <main><section class="projects">  <div class="container">    <h2 class="title projects__title">TrainSet Academy</h2>    <div class="cardSection">      <div class="navigationSlider">        <div class="navigationSlider__arrow swiper-button-prev"></div>        <div class="navigationSlider__container swiper-container">          <div class="navigationSlider__wrapper swiper-wrapper">            <a href="#" class="navigationSlider__link swiper-slide active">Doks</a>            <a href="#" class="navigationSlider__link swiper-slide">Blog</a>            <a href="#" class="navigationSlider__link swiper-slide">Training</a>          </div>        </div>        <div class="navigationSlider__arrow swiper-button-next"></div>      </div>      <div class="navUlListContetnt">  <h2 class="navUlListContetnt__title">SUPERVISED LEARNING</h2>  <div class="navUlListContetnt__row">    <div class="navUlListContetnt__linksWrapper">      <a href="#" class="navUlListContetnt__link navUlListContetnt__link_big">REGRESSION</a>      <a href="#" class="navUlListContetnt__link navUlListContetnt__link_active">Lasso, Ridge & Elastic Net</a>      <a href="#" class="navUlListContetnt__link">Linear Regression</a>      <a href="#" class="navUlListContetnt__link">KNN Regression</a>      <a href="#" class="navUlListContetnt__link">Multivariate Regression</a>      <a href="#" class="navUlListContetnt__link">Neural Network Regressor</a>      <a href="#" class="navUlListContetnt__link">Polynomial Regression</a>    </div>    <h6 class="cardSection__title navUlListContetnt__responsiveTitle-js">Lasso, Ridge & Elastic Net</h6>    <div class="author">  <div class="author__image">    <picture><source srcset="./resources/images/academy/avatar_1.webp" type="image/webp"><img src="./resources/images/academy/avatar_1.png" alt="Andrew Wolf" /></picture>  </div>  <div class="author__column">    <p class="author__text">Written by <span class="author__text_bold">Victor Popov</span></p>    <p class="author__text">Feb 29, 2020</p>  </div></div>  </div></div>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">1. Introduction to Lasso Regularization Term (L1)</h6>        <p class="paragraph__text">          LASSO - Least Absolute Shrinkage and Selection Operator - was first formulated by Robert Tibshirani in 1996. It is a          powerful method that performs two main tasks: regularization and feature selection.        </p>        <p class="paragraph__text">          Let’s look at the example of lasso regularization with linear models, where OLS method is used with its regularization          term.        </p>        <div class="nestedPicture cardSection__nestedPicture nestedPicture_math-code">          <div class="nestedPicture__image"><picture><source srcset="./resources/images/academy/1.webp" type="image/webp"><img src="./resources/images/academy/1.png" alt="#" /></picture></div>        </div>        <p class="paragraph__text">          The LASSO method puts a constraint on the sum of the absolute values of the model parameters, the sum has to be less          than a fixed value (upper bound, or t):        </p>        <div class="paragraph__text paragraph__text_math-code">          <math class="paragraph__text paragraph__text_math-code" xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <munderover>              <mo>&#x2211;<!-- ∑ --></mo>              <mrow class="MJX-TeXAtom-ORD">                <mi>k</mi>              </mrow>              <mrow class="MJX-TeXAtom-ORD">                <mi>j</mi>                <mo>=</mo>                <mn>1</mn>              </mrow>            </munderover>            <mrow>              <mo>|</mo>              <msub>                <mi>&#x03B2;<!-- β --></mi>                <mrow class="MJX-TeXAtom-ORD">                  <mi>j</mi>                </mrow>              </msub>              <mo>|</mo>            </mrow>            <mo>&lt;</mo>            <mi>t</mi>            <mo>,</mo>          </math>        </div>        <p class="paragraph__text">where t is the upper bound for the sum of the coefficients.</p>        <p class="paragraph__text">          In order to do so, the method applies a shrinking (regularization) process where it penalizes the coefficients of the          regression variables shrinking some of them to zero. During features selection process the variables that still have a          non-zero coefficient after the shrinking process are selected to be part of the model. The goal of this process is to          minimize the prediction error.        </p>      </div>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">2. Parameter</h6>        <div class="paragraph__text paragraph__text_math-code">          <math class="paragraph__text paragraph__text_math-code" xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <mi>&#x03B1;<!-- α --></mi>          </math>        </div>        <p class="paragraph__text">          In practice, the tuning parameter α that controls the strength of the penalty assumes great importance. Indeed, when α          is sufficiently large, coefficients are forced to be exactly equal to zero. This way, dimensionality can be reduced. The          larger the parameter α, the more the number of coefficients are shrunk to zero. On the other hand, if α = 0, we have          just an OLS (Ordinary Least Squares) regression.        </p>      </div>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">3. Advanages</h6>        <p class="paragraph__text">          In practice, the tuning parameter α that controls the strength of the penalty assumes great importance. Indeed, when α          is sufficiently large, coefficients are forced to be exactly equal to zero. This way, dimensionality can be reduced. The          larger the parameter α, the more the number of coefficients are shrunk to zero. On the other hand, if α = 0, we have          just an OLS (Ordinary Least Squares) regression.        </p>        <ul class="paragraph__list paragraph__text paragraph__list_dots-type">          <li>            First of all, it can provide a very good prediction accuracy, because shrinking and removing the coefficients can            reduce variance without a substantial increase of the bias, this is especially useful when you have a small number of            observation and a large number of features. In terms of the tuning parameter α we know that bias increases and            variance decreases when α increases, indeed a trade-off between bias and variance has to be found;          </li>          <li>            Moreover, the LASSO helps to increase the model interpretability by eliminating irrelevant variables that are not            associated with the response variable, this way also overfitting is reduced. This is the point where we are more            interested in because in this paper the focus is on the feature selection task.          </li>        </ul>      </div>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">4. Introduction to Lasso Regression</h6>        <p class="paragraph__text">          Lasso with linear models is called Lasso Regression. It is the model that describes the relationship between response          variable Y and explanatory variables X. In the case of one explanatory variable, Lasso Regression is called Simple Lasso          Regression while the case with two or more explanatory variables is called Multiple Lasso Regression.        </p>        <p class="paragraph__text">Lasso Regression holds all the assumptions of the Linear Regression, such as:</p>        <ul class="paragraph__list paragraph__text paragraph__list_dots-type">          <li class="">The response variable is normally distributed;</li>          <li>There is a linear relationship between the response variable and the explanatory variables;</li>          <li>            The random errors are normally distributed, have constant (equal) variances at any point in X, and are independent.          </li>        </ul>        <div>          <p class="paragraph__text">            <span class="paragraph__text_italic">To read more about Linear Regression assumptions</span>, go to            <a href="https://www.thelearningmachine.ai/cnn" class="paragraph__text_link">Linear Regression</a>          </p>        </div>      </div>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">5. The Model</h6>        <p class="paragraph__text">          The LASSO minimizes the sum of squared errors, with an upper bound on the sum of the absolute values of the model          parameters. The lasso estimate is defined by the solution to the L1 optimization problem:        </p>        <div class="paragraph__text paragraph__text_math-code">          <math class="paragraph__text paragraph__text_math-code" xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <mo>&lt;</mo>            <mi>i</mi>            <mo>&gt;</mo>            <mi>m</mi>            <mi>i</mi>            <mi>n</mi>            <mi>i</mi>            <mi>m</mi>            <mi>i</mi>            <mi>z</mi>            <mi>e</mi>            <mo>&lt;</mo>            <mrow class="MJX-TeXAtom-ORD">              <mo>/</mo>            </mrow>            <mi>i</mi>            <mo>&gt;</mo>            <mrow>              <mo>[</mo>              <mfrac>                <mrow>                  <munderover>                    <mo>&#x2211;<!-- ∑ --></mo>                    <mrow class="MJX-TeXAtom-ORD">                      <mi>&#x0131;<!-- ı --></mi>                      <mo>=</mo>                      <mn>1</mn>                    </mrow>                    <mrow class="MJX-TeXAtom-ORD">                      <mi>n</mi>                    </mrow>                  </munderover>                  <mo stretchy="false">(</mo>                  <msub>                    <mi>y</mi>                    <mrow class="MJX-TeXAtom-ORD">                      <mi>i</mi>                    </mrow>                  </msub>                  <mo>&#x2212;<!-- − --></mo>                  <mo stretchy="false">)</mo>                  <mo stretchy="false">(</mo>                  <msub>                    <mi>&#x03B2;<!-- β --></mi>                    <mrow class="MJX-TeXAtom-ORD">                      <mi>i</mi>                    </mrow>                  </msub>                  <msub>                    <mi>x</mi>                    <mrow class="MJX-TeXAtom-ORD">                      <mi>i</mi>                    </mrow>                  </msub>                  <mo>+</mo>                  <msub>                    <mi>&#x03B2;<!-- β --></mi>                    <mrow class="MJX-TeXAtom-ORD">                      <mn>0</mn>                    </mrow>                  </msub>                  <mo stretchy="false">)</mo>                  <msup>                    <mo stretchy="false">)</mo>                    <mn>2</mn>                  </msup>                </mrow>                <mi>n</mi>              </mfrac>              <mo>]</mo>            </mrow>            <mo>&lt;</mo>            <mi>i</mi>            <mo>&gt;</mo>            <mi>s</mi>            <mi>u</mi>            <mi>b</mi>            <mi>j</mi>            <mi>e</mi>            <mi>c</mi>            <mi>t</mi>            <mi>t</mi>            <mi>o</mi>            <mo>&lt;</mo>            <mrow class="MJX-TeXAtom-ORD">              <mo>/</mo>            </mrow>            <mi>i</mi>            <mo>&gt;</mo>            <mrow>              <mo>[</mo>              <mrow>                <mi>&#x03B1;<!-- α --></mi>                <munderover>                  <mo>&#x2211;<!-- ∑ --></mo>                  <mrow class="MJX-TeXAtom-ORD">                    <mi>&#x0237;<!-- ȷ --></mi>                    <mo>=</mo>                    <mn>1</mn>                  </mrow>                  <mrow class="MJX-TeXAtom-ORD">                    <mi>k</mi>                  </mrow>                </munderover>                <mrow>                  <mo>|</mo>                  <msub>                    <mi>&#x03B2;<!-- β --></mi>                    <mrow class="MJX-TeXAtom-ORD">                      <mi>&#x0237;<!-- ȷ --></mi>                    </mrow>                  </msub>                  <mo>|</mo>                </mrow>                <mo>&lt;</mo>                <mi>t</mi>              </mrow>              <mo>]</mo>            </mrow>            <mo>,</mo>          </math>        </div>        <p class="paragraph__text">          where <span class="paragraph__text_bold">t </span>is the upper bound for the sum of the coefficients, n is the number of          response variables and        </p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <mi>&#x03B1;<!-- α --></mi>            <mo>&#x2265;<!-- ≥ --></mo>            <mn>0</mn>          </math>        </div>        <p class="paragraph__text">          is the parameter that controls the strength of the penalty, the larger the value of α, the greater the amount of          shrinkage. This optimization problem is equivalent to the parameter estimation that follows:        </p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <mrow class="MJX-TeXAtom-ORD">              <mover>                <mi>&#x03B2;<!-- β --></mi>                <mo stretchy="false">&#x005E;<!-- ^ --></mo>              </mover>            </mrow>            <mo stretchy="false">(</mo>            <mi>&#x03B1;<!-- α --></mi>            <mo stretchy="false">)</mo>            <mo>=</mo>            <munder>              <mrow>                <mi>a</mi>                <mi>r</mi>                <mi>g</mi>                <mi>m</mi>                <mi>i</mi>                <mi>n</mi>              </mrow>              <mi>&#x03B2;<!-- β --></mi>            </munder>            <mrow>              <mo>[</mo>              <mrow>                <mfrac>                  <mrow>                    <munderover>                      <mo>&#x2211;<!-- ∑ --></mo>                      <mrow class="MJX-TeXAtom-ORD">                        <mi>i</mi>                        <mo>=</mo>                        <mn>1</mn>                      </mrow>                      <mrow class="MJX-TeXAtom-ORD">                        <mi>n</mi>                      </mrow>                    </munderover>                    <mo stretchy="false">(</mo>                    <msub>                      <mi>y</mi>                      <mrow class="MJX-TeXAtom-ORD">                        <mi>i</mi>                      </mrow>                    </msub>                    <mo>&#x2212;<!-- − --></mo>                    <mo stretchy="false">(</mo>                    <msub>                      <mi>&#x03B2;<!-- β --></mi>                      <mrow class="MJX-TeXAtom-ORD">                        <mi>i</mi>                      </mrow>                    </msub>                    <msub>                      <mi>x</mi>                      <mrow class="MJX-TeXAtom-ORD">                        <mi>i</mi>                      </mrow>                    </msub>                    <mo>+</mo>                    <msub>                      <mi>&#x03B2;<!-- β --></mi>                      <mrow class="MJX-TeXAtom-ORD">                        <mn>0</mn>                      </mrow>                    </msub>                    <mo stretchy="false">)</mo>                    <msup>                      <mo stretchy="false">)</mo>                      <mn>2</mn>                    </msup>                  </mrow>                  <mi>n</mi>                </mfrac>                <mo>+</mo>                <mi>&#x03B1;<!-- α --></mi>                <munderover>                  <mo>&#x2211;<!-- ∑ --></mo>                  <mrow class="MJX-TeXAtom-ORD">                    <mi>j</mi>                    <mo>=</mo>                    <mn>1</mn>                  </mrow>                  <mrow class="MJX-TeXAtom-ORD">                    <mi>k</mi>                  </mrow>                </munderover>                <mrow>                  <mo>|</mo>                  <msub>                    <mi>&#x03B2;<!-- β --></mi>                    <mrow class="MJX-TeXAtom-ORD">                      <mi>j</mi>                    </mrow>                  </msub>                  <mo>|</mo>                </mrow>              </mrow>              <mo>]</mo>            </mrow>          </math>        </div>        <p class="paragraph__text">          The relation between α and the upper bound t is a reverse relationship. We already know that α controls the strength of          the penalty. When α is large, coefficients are forced to be exactly equal to zero, and when α = 0, we have just an OLS          (Ordinary Least Squares) method, which estimates parameters.        </p>        <p class="paragraph__text">          When <span class="paragraph__text_bold">t</span> becomes close to 0, let us say 0.00001 (meaning that the absolute sum          of all coefficients should be less than 0.00001), α goes to infinity as it forces coefficients to be exactly 0. On the          contrary, as <span class="paragraph__text_bold">t</span> becomes infinity (meaning that the absolute sum of all          coefficients should be less than infinity), α becomes 0, as there is no urgency to shrink coefficients, so the problem          becomes just an ordinary least squares.        </p>      </div>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">6. Lasso Regression in Python</h6>        <p class="paragraph__text">          View/download a template of Lasso Regression located in a git repository          <a href="https://github.com/5x12/ML-Cookbook/blob/master/Regression/lasso_regression.ipynb" class="paragraph__text_link"            >here</a          >        </p>      </div>      <h6 class="cardSection__title">Ridge Regression</h6>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">1. Introduction to Ridge Regularization Term (L2)</h6>        <p class="paragraph__text">          Ridge Regression uses OLS method, but with one difference: it has a regularization term\alpha          \sum_{j=1}^{p}w_{j}^{2}(also known as L2 penalty or penalty term).L2 penalty or penalty term        </p>        <p class="paragraph__text">          Ridge Regression allows you to regularize coefficients. This means that the estimated coefficients are pushed towards 0,          to make them work better on new data-sets ("optimized for prediction"). This allows you to use complex models and avoid          over-fitting at the same time.        </p>        <p class="paragraph__text">For Ridge Regression you have to set an</p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <mi>&#x03B1;<!-- α --></mi>          </math>        </div>        <p class="paragraph__text">          ("alpha") - a so-called "meta-parameter" (or "regularization parameter") that defines how aggressive regularization is          performed. Alpha simply defines regularization strength and is usually chosen by cross-validation.        </p>        <p class="paragraph__text">if</p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <mi>&#x03B1;<!-- α --></mi>          </math>        </div>        <p class="paragraph__text">is too large,</p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <msub>              <mi>h</mi>              <mrow class="MJX-TeXAtom-ORD">                <mi>v</mi>              </mrow>            </msub>            <mo stretchy="false">(</mo>            <mi>x</mi>            <mo stretchy="false">)</mo>            <mo>&#x2248;<!-- ≈ --></mo>            <msub>              <mi>w</mi>              <mrow class="MJX-TeXAtom-ORD">                <mn>0</mn>              </mrow>            </msub>          </math>        </div>        <p class="paragraph__text">and thus</p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <msub>              <mi>w</mi>              <mrow class="MJX-TeXAtom-ORD">                <mi>j</mi>              </mrow>            </msub>            <mo>&#x2248;<!-- ≈ --></mo>            <mn>0</mn>          </math>        </div>        <p class="paragraph__text">causing an underfitting.</p>        <p class="paragraph__text">          Regularization works especially well when you have a relatively small amount of training data compared to the number of          features in a model. It becomes less important as the amount of training data increases.        </p>      </div>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">2. Feature Normalization</h6>        <p class="paragraph__text">          Feature scaling is very important in Ridge Regression: input variables with different scales will have different          contributions to L2 penalty. Transform input features so that L2 penalty is applied more fairly to all features (without          weighting some more than others just because of the difference in scales):        </p>        <ul class="paragraph__list paragraph__text paragraph__list_dots-type">          <li>Fit the scaler using the training set, then apply the same scaler to transform the test set;</li>          <li>Do not scale the training and test sets using different scalers: this could lead to a random skew in the data;</li>          <li>Note that the resulting model and the transferred features may be harder to interpret.</li>        </ul>      </div>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">3. Ridge Regression in Python</h6>        <p class="paragraph__text">          View/download a template of Ridge Regression located in a git repository          <a class="paragraph__text_link" href="https://github.com/5x12/ML-Cookbook/blob/master/Regression/ridge_regression.ipynb"            >here</a          >        </p>      </div>      <h6 class="cardSection__title">Ridge VS. Lasso</h6>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">1. Introduction</h6>        <p class="paragraph__text">          In this section, the difference between Lasso and Ridge regression models is outlined. We assume you to know both Ridge          and Lasso regressions described above.        </p>        <p class="paragraph__text">          Ridge regression is an extension for linear regression. It’s basically a regularized linear regression model. The        </p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <mi>&#x03B1;<!-- α --></mi>          </math>        </div>        <p class="paragraph__text">          parameter is a scalar that should be learned as well, using a method called cross-validation.        </p>        <p class="paragraph__text">          An extremely important fact we need to notice about ridge regression is that it enforces the        </p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <mi>&#x03B2;<!-- β --></mi>          </math>        </div>        <p class="paragraph__text">          coefficients to be lower, but it does not enforce them to be zero. That is, it will not get rid of irrelevant features          but rather minimize their impact on the trained model.        </p>        <p class="paragraph__text">          The only difference from Ridge regression is that the regularization term is          <span class="paragraph__text_bold">in absolute value</span>. But this difference has a huge impact on the trade-off          we’ve discussed before. Lasso method overcomes the disadvantage of Ridge regression by not only punishing high values of          the coefficients β but actually setting them to zero if they are not relevant. Therefore, you might end up with fewer          features included in the model than you started with, which is a huge advantage.        </p>        <p class="paragraph__text">          Keep in mind that Ridge regression <span class="paragraph__text_bold">cannot zero out coefficients;</span> thus, you          either end up including all the coefficients in the model or none of them. In contrast, the LASSO does both          <span class="paragraph__text_bold">parameter shrinkage</span> and          <span class="paragraph__text_bold">variable selection automatically</span>. If some of your covariates are highly          correlated, you may want to look at the Elastic Net instead of the LASSO.        </p>      </div>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">2. Why Lasso Shrinks Coefficients</h6>        <p class="paragraph__text">          The main difference between ridge and lasso regression is a shape of their constraint regions. Ridge regression use L2          norm for a constraint. For P= 2 (where P is a number of regressors) case, the shape of the constraint region is a          <span class="paragraph__text_bold">circle</span>. Lasso uses L1 norm for a constraint. For P = 2 case, the shape of the          constraint region is a <span class="paragraph__text_bold">diamond</span>.        </p>        <p class="paragraph__text">          The elliptical contour plot in the figure represents sum of squares error term. The Lasso estimate is an estimate which          minimizes the sum of squares as well as satisfies its "diamond" constraint. The Ridge estimate is an estimate which          minimizes the sum of squares as well as satisfies its "circle" constraint.        </p>        <p class="paragraph__text">          Thus, the optimal point is a point which is a common point between an ellipse and L1/L2 constraint. This point tries to          find the minimum for the constraint that will work for the regression model. Exactly that point gives a minimum value          for the Ridge or Lasso function.        </p>        <ul class="paragraph__list paragraph__text paragraph__list_dots-type">          <li>            For the LASSO method the constraint region is a diamond, thus it has corners; Because it has corners, there is a high            probability that optimum point (minimum point) falls in the corner point of the diamond region. For P=2 case, if an            optimal point falls in the corner point, it means that one of the feature estimate is zero.          </li>          <li class="paragraph__text_math-code">            <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">              <msub>                <mi>&#x03B2;<!-- β --></mi>                <mrow class="MJX-TeXAtom-ORD">                  <mi>j</mi>                </mrow>              </msub>              <mo>=</mo>              <mn>0</mn>            </math>          </li>          <li>            For the RIDGE method the constraint region is a disk, thus it has no corners and the coefficients cannot be equal to            zero, as point minimum will be located elsewhere.          </li>        </ul>      </div>      <h6 class="cardSection__title">Elasic Net</h6>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">1. Introduction</h6>        <p class="paragraph__text">Elastic Net is a method that includes both Lasso and Ridge.</p>        <p class="paragraph__text">The LASSO method has some limitations:</p>        <ul class="paragraph__list paragraph__text paragraph__list_dots-type">          <li>            In small-n-large-p dataset (high-dimensional data with few examples), the LASSO selects at most n variables before it            saturates;          </li>          <li>            If there is a group of highly correlated variables, LASSO tends to select one variable from a group and ignore the            others.          </li>        </ul>        <p class="paragraph__text">          To overcome these limitations, the elastic net adds a quadratic part to the L1 penalty, which when used alone is a ridge          regression (known also as Tikhonov regularization or L2). The estimates from the elastic net method are defined by        </p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <mrow class="MJX-TeXAtom-ORD">              <mover>                <mi>&#x03B2;<!-- β --></mi>                <mo>&#x005E;<!-- ^ --></mo>              </mover>            </mrow>            <mo stretchy="false">(</mo>            <mi>&#x03B1;<!-- α --></mi>            <mo stretchy="false">)</mo>            <mo>=</mo>            <munder>              <mrow>                <mi>a</mi>                <mi>r</mi>                <mi>g</mi>                <mi>m</mi>                <mi>i</mi>                <mi>n</mi>              </mrow>              <mi>&#x03B2;<!-- β --></mi>            </munder>            <mrow>              <mo>[</mo>              <mrow>                <mfrac>                  <mrow>                    <munderover>                      <mo>&#x2211;<!-- ∑ --></mo>                      <mrow class="MJX-TeXAtom-ORD">                        <mi>i</mi>                        <mo>=</mo>                        <mn>1</mn>                      </mrow>                      <mrow class="MJX-TeXAtom-ORD">                        <mi>n</mi>                      </mrow>                    </munderover>                    <mo stretchy="false">(</mo>                    <msub>                      <mi>y</mi>                      <mrow class="MJX-TeXAtom-ORD">                        <mi>i</mi>                      </mrow>                    </msub>                    <mo>&#x2212;<!-- − --></mo>                    <mo stretchy="false">(</mo>                    <msub>                      <mi>&#x03B2;<!-- β --></mi>                      <mrow class="MJX-TeXAtom-ORD">                        <mi>i</mi>                      </mrow>                    </msub>                    <msub>                      <mi>x</mi>                      <mrow class="MJX-TeXAtom-ORD">                        <mi>i</mi>                      </mrow>                    </msub>                    <mo>+</mo>                    <msub>                      <mi>&#x03B2;<!-- β --></mi>                      <mrow class="MJX-TeXAtom-ORD">                        <mn>0</mn>                      </mrow>                    </msub>                    <mo stretchy="false">)</mo>                    <msup>                      <mo stretchy="false">)</mo>                      <mn>2</mn>                    </msup>                  </mrow>                  <mi>n</mi>                </mfrac>                <mo>+</mo>                <msub>                  <mi>&#x03B1;<!-- α --></mi>                  <mrow class="MJX-TeXAtom-ORD">                    <mn>1</mn>                  </mrow>                </msub>                <munderover>                  <mo>&#x2211;<!-- ∑ --></mo>                  <mrow class="MJX-TeXAtom-ORD">                    <mi>j</mi>                    <mo>=</mo>                    <mn>1</mn>                  </mrow>                  <mrow class="MJX-TeXAtom-ORD">                    <mi>k</mi>                  </mrow>                </munderover>                <mrow>                  <mo>|</mo>                  <msub>                    <mi>&#x03B2;<!-- β --></mi>                    <mrow class="MJX-TeXAtom-ORD">                      <mi>j</mi>                    </mrow>                  </msub>                  <mo>|</mo>                </mrow>                <mo>+</mo>                <msub>                  <mi>&#x03B1;<!-- α --></mi>                  <mrow class="MJX-TeXAtom-ORD">                    <mn>2</mn>                  </mrow>                </msub>                <munderover>                  <mo>&#x2211;<!-- ∑ --></mo>                  <mrow class="MJX-TeXAtom-ORD">                    <mi>j</mi>                    <mo>=</mo>                    <mn>1</mn>                  </mrow>                  <mrow class="MJX-TeXAtom-ORD">                    <mi>k</mi>                  </mrow>                </munderover>                <mo stretchy="false">(</mo>                <msub>                  <mi>&#x03B2;<!-- β --></mi>                  <mrow class="MJX-TeXAtom-ORD">                    <mi>j</mi>                  </mrow>                </msub>                <msup>                  <mo stretchy="false">)</mo>                  <mn>2</mn>                </msup>              </mrow>              <mo>]</mo>            </mrow>          </math>        </div>        <p class="paragraph__text paragraph__text_italic paragraph__text_centered">where</p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <msub>              <mi>&#x03B1;<!-- α --></mi>              <mrow class="MJX-TeXAtom-ORD">                <mn>1</mn>              </mrow>            </msub>            <mo>&#x2A7E;<!-- ⩾ --></mo>            <mn>0</mn>          </math>        </div>        <p class="paragraph__text paragraph__text_italic paragraph__text_centered">and</p>        <div class="paragraph__text paragraph__text_math-code">          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">            <msub>              <mi>&#x03B1;<!-- α --></mi>              <mrow class="MJX-TeXAtom-ORD">                <mn>2</mn>              </mrow>            </msub>            <mo>&#x2A7E;<!-- ⩾ --></mo>            <mn>0</mn>          </math>        </div>        <p class="paragraph__text paragraph__text_italic paragraph__text_centered">are two regularization parameters.</p>      </div>      <div class="paragraph cardSection__paragraph">        <h6 class="paragraph__title paragraph__title_BG">2. Comparing L1 & L2 with Elastic Net</h6>        <p class="paragraph__text">Consider the plots of the abs and square functions.</p>        <p class="paragraph__text">          When minimizing a loss function with a regularization term, each of the entries in the parameter vector theta are          “pulled” down towards zero. Think of each entry in theta lying on one the above curves and being subjected to “gravity”          proportional to the regularization hyperparameter k. In the context of L1-regularization, the entries of theta are          pulled towards zero proportionally to their absolute values — they lie on the red curve.        </p>      </div>    </div>    <div class="fixedArrow">      <a href="#startPage" class="fixedArrow__arrow">up</a>    </div>  </div></section></main>      <footer class="footer">  <div class="footer__wrapper">    <div class="footer__row">      <a href="#" class="footer__link">Home</a>      <a href="#" class="footer__link">Blog</a>      <a href="#" class="footer__link">Contact</a>    </div>    <div class="footer__row">      <a href="https://www.facebook.com/thelearningm" class="footer__socialMedia"  target="_blank"><picture><source srcset="./resources/icons/facebook-icon.svg" type="image/webp"><img src="./resources/icons/facebook-icon.svg" alt="facebook" /></picture></a>      <a href="https://www.instagram.com/thelearningm" target="_blank" class="footer__socialMedia"><picture><source srcset="./resources/icons/instagram-icon.svg" type="image/webp"><img src="./resources/icons/instagram-icon.svg" alt="instagram" /></picture></a>    </div>  </div></footer>    </div>    <!-- swiper -->    <script src="./js/js__plugins/swiper-bundle.min.js"></script>    <!-- main js -->    <script src="./js/main.js"></script>  </body></html>